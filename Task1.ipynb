{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Natural Language Processing Toolkit\n",
    "\n",
    "- Build gradio UIs to apply transformer models in different\n",
    "NLP contexts (Named Entity Recognition, Translation,\n",
    "Summarization)\n",
    "- Create and invite me to a GitHub repository for task 1\n",
    "- Coding and presentation should be based on Jupyter\n",
    "Notebook files (ipynb)\n",
    "- Additional record and upload a screencast demonstrating\n",
    "your application\n",
    "- Task Share: 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we are utilizing several powerful Python libraries to perform data processing, , named entity recognition, translation, text summarization, and to create a user-friendly interface. Below are the descriptions of each imported library:\n",
    "\n",
    "> - re: This is the regular expression library in Python. It provides a set of functions that allows us to search, match, and manipulate strings using regular expressions. Regular expressions are used for string searching and manipulation, which can be very useful for data cleaning and preprocessing.\n",
    "\n",
    "> - pandas (pd): Pandas is a highly popular data manipulation and analysis library for Python. It provides data structures like DataFrame, which is used for handling and analyzing structured data efficiently. In this script, we use pandas to read and preprocess the CSV file containing articles and their highlights.\n",
    "\n",
    "> - gradio (gr): Gradio is a library that allows you to create user-friendly web interfaces for your machine learning models easily. It helps in building interactive demos that can be shared with others without any web development knowledge. In this script, Gradio will be used to create an interactive interface for the text summarization model.\n",
    "\n",
    "> - transformers (pipeline, AutoTokenizer): Transformers is a library by Hugging Face that provides state-of-the-art machine learning models, especially for natural language processing (NLP). The pipeline function simplifies the usage of these models by providing high-level APIs for common tasks like text summarization. AutoTokenizer is used to load the appropriate tokenizer corresponding to the chosen model, which is essential for preparing text data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from transformers import pipeline, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we handle the import and preparation of datasets for the natural language processing (NLP) tasks named entity recognition (NER), translation and summarization. Each task requires its specific dataset, which we read and preprocess accordingly. This setup ensures that our data is structured and ready for the intended NLP analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we handle the import and preparation of data specifically for the Named Entity Recognition (NER) task. We load a text file containing sentences from Wikipedia crawls and map each sentence to a descriptive label. This mapping helps in organizing the sentences by their context, which can be useful for understanding the content and performing NER tasks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The modern Olympic Games or Olympics (French: Jeux olympiques)[a][1] are the leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the named entity recognition data from a file\n",
    "with open('Case Dataset/Data files/NER_text_Wikipedia_crawl.txt', 'r') as file:\n",
    "    named_entity_recognition_data = file.read()\n",
    "\n",
    "def map_sentences_to_descriptions(file):\n",
    "    \"\"\"\n",
    "    Maps sentences from a given text file to descriptive labels.\n",
    "\n",
    "    Args:\n",
    "        file (str): The content of the text file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping descriptive labels to sentences.\n",
    "    \"\"\"\n",
    "    # Basic sentence splitting using regular expressions\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', file)\n",
    "    \n",
    "    # Provide a descriptive word for each sentence\n",
    "    descriptions = [\n",
    "        \"Introduction\", \"Importance\", \"Frequency\", \"Inspiration\", \"Founding\",\n",
    "        \"Governance\", \"Evolution\", \"Adjustments\", \"Endorsements\", \"Adaptation\",\n",
    "        \"Changes\", \"Media\", \"Cancellations\", \"Components\", \"Responsibilities\",\n",
    "        \"Programme\", \"Symbols\", \"Participation\", \"Awards\", \"Growth\",\n",
    "        \"Challenges\", \"Exposure\", \"Showcase\", \"Ancient\", \"Legend\",\n",
    "        \"Myth\", \"Tradition\", \"Religious\", \"Decline\", \"Revival\", \"Modern\",\n",
    "        \"Forerunners\", \"National\", \"Festival\", \"Event\", \"Reconstruction\",\n",
    "        \"Historic\", \"Legacy\", \"Promotion\", \"Development\", \"Re-establishment\",\n",
    "        \"Success\", \"Athens\", \"Paris\", \"Stagnation\", \"Survival\", \"Rebounding\",\n",
    "        \"Popularity\", \"Winter\", \"Figure Skating\", \"Ice Hockey\", \"Expansion\",\n",
    "        \"Congress\", \"Host\", \"International\", \"Agreement\", \"Official\", \"Youth\",\n",
    "        \"Games\", \"Opportunities\", \"Nonprofit\", \"Effect\", \"Displacement\",\n",
    "        \"Business\", \"Infrastructure\", \"Sponsorship\", \"Revenue\", \"Expenditures\",\n",
    "        \"Financial\", \"Profit\", \"Exclusivity\", \"Marketing\", \"Broadcasting\",\n",
    "        \"Audience\", \"Television\", \"Viewership\", \"Commercialisation\",\n",
    "        \"Economic\", \"Investment\", \"International\", \"Symbolism\", \"Flag\",\n",
    "        \"Motto\", \"Flame\", \"Mascot\", \"Ceremony\", \"Parade\", \"Athletes\",\n",
    "        \"Hosts\", \"Medals\", \"Victory\", \"Events\", \"Governing\", \"Demonstration\",\n",
    "        \"Recognized\", \"Professionalism\", \"Controversy\", \"Amateurism\",\n",
    "        \"Participation\", \"Boycotts\", \"Politics\", \"Protest\", \"Doping\", \"Scandal\",\n",
    "        \"Testing\", \"Banning\", \"Citizenship\", \"Medallists\", \"Athletes\",\n",
    "        \"Nations\", \"Hosting\"\n",
    "    ]\n",
    "\n",
    "    # Ensure the descriptions list matches the number of sentences\n",
    "    # If there are more sentences, repeat the last description\n",
    "    while len(descriptions) < len(sentences):\n",
    "        descriptions.append(descriptions[-1])\n",
    "\n",
    "    # Create a dictionary mapping sentences to descriptions\n",
    "    sentence_description_map = {description: sentence.strip() for description, sentence in zip(descriptions, sentences)}\n",
    "\n",
    "    return sentence_description_map\n",
    "\n",
    "# Map the named entity recognition data to descriptions\n",
    "named_entity_recognition_data = map_sentences_to_descriptions(named_entity_recognition_data)\n",
    "\n",
    "named_entity_recognition_data['Introduction']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we handle the import and preparation of data specifically for the translation task. We load a CSV file containing the translation training data, adjust the column names to remove any locale-specific suffixes, and prepare the data for translation. This step ensures that the data is in a consistent format and ready for the translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1847</td>\n",
       "      <td>train</td>\n",
       "      <td>order me a cheese burger from tommy's burgers</td>\n",
       "      <td>bestell mir einen cheeseburger von tommy's bur...</td>\n",
       "      <td>pídeme una hamburguesa de queso del mcdonalds</td>\n",
       "      <td>commande moi un burger au fromage chez tommy's...</td>\n",
       "      <td>ordinami un cheese burger da america graffiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>876</td>\n",
       "      <td>train</td>\n",
       "      <td>play kari jobe for me</td>\n",
       "      <td>spiel kari jobe für mich</td>\n",
       "      <td>pon melendi para mi</td>\n",
       "      <td>mets jacques brel ne me quitte pas</td>\n",
       "      <td>metti laura pausini per me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14494</td>\n",
       "      <td>train</td>\n",
       "      <td>what is i. b. m.'s stock worth</td>\n",
       "      <td>was ist i. b. m.'s aktie wert</td>\n",
       "      <td>cuál es el valor de las acciones del ibm</td>\n",
       "      <td>quelle est la valeur des actions d'i. b. m.</td>\n",
       "      <td>qual è il valore delle azioni generali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14366</td>\n",
       "      <td>train</td>\n",
       "      <td>will it be good to buy nike stock today</td>\n",
       "      <td>wäre es gut heute volkswagen aktien zu kaufen</td>\n",
       "      <td>será bueno comprar acciones de nike hoy dia</td>\n",
       "      <td>sera-t-il bon d'acheter des actions nike aujou...</td>\n",
       "      <td>oggi è un buon giorno per comprare le azioni d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1977</td>\n",
       "      <td>train</td>\n",
       "      <td>please remove the alarm which i set for today ...</td>\n",
       "      <td>bitte lösche den wecker den ich für heute früh...</td>\n",
       "      <td>por favor borrar la alarma que tenía activada ...</td>\n",
       "      <td>veuillez retirer l'alarme que j'ai réglée pour...</td>\n",
       "      <td>rimuovi la sveglia impostata per questa mattina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  split                                                 en  \\\n",
       "0   1847  train      order me a cheese burger from tommy's burgers   \n",
       "1    876  train                              play kari jobe for me   \n",
       "2  14494  train                     what is i. b. m.'s stock worth   \n",
       "3  14366  train            will it be good to buy nike stock today   \n",
       "4   1977  train  please remove the alarm which i set for today ...   \n",
       "\n",
       "                                                  de  \\\n",
       "0  bestell mir einen cheeseburger von tommy's bur...   \n",
       "1                           spiel kari jobe für mich   \n",
       "2                      was ist i. b. m.'s aktie wert   \n",
       "3      wäre es gut heute volkswagen aktien zu kaufen   \n",
       "4  bitte lösche den wecker den ich für heute früh...   \n",
       "\n",
       "                                                  es  \\\n",
       "0      pídeme una hamburguesa de queso del mcdonalds   \n",
       "1                                pon melendi para mi   \n",
       "2           cuál es el valor de las acciones del ibm   \n",
       "3        será bueno comprar acciones de nike hoy dia   \n",
       "4  por favor borrar la alarma que tenía activada ...   \n",
       "\n",
       "                                                  fr  \\\n",
       "0  commande moi un burger au fromage chez tommy's...   \n",
       "1                 mets jacques brel ne me quitte pas   \n",
       "2        quelle est la valeur des actions d'i. b. m.   \n",
       "3  sera-t-il bon d'acheter des actions nike aujou...   \n",
       "4  veuillez retirer l'alarme que j'ai réglée pour...   \n",
       "\n",
       "                                                  it  \n",
       "0      ordinami un cheese burger da america graffiti  \n",
       "1                         metti laura pausini per me  \n",
       "2             qual è il valore delle azioni generali  \n",
       "3  oggi è un buon giorno per comprare le azioni d...  \n",
       "4    rimuovi la sveglia impostata per questa mattina  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load translation training data from a CSV file\n",
    "translation_data = pd.read_csv('Case Dataset/Data files/Translation_Training.csv', sep=';')\n",
    "\n",
    "def adjust_column_names(df):\n",
    "    \"\"\"\n",
    "    Adjusts the column names of a DataFrame by removing the locale suffix.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame whose column names need adjustment.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with adjusted column names.\n",
    "    \"\"\"\n",
    "    df.columns = [column.split('_')[0] for column in df.columns]\n",
    "    return df\n",
    "\n",
    "# Apply the column name adjustment function to the translation data\n",
    "translation_data = adjust_column_names(translation_data)\n",
    "\n",
    "translation_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we handle the import of data specifically for the summarization task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 ...</td>\n",
       "      <td>John and .\\nAudrey Cook were discovered alongs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNITED NATIONS (CNN) -- A rare meeting of U.N....</td>\n",
       "      <td>NEW: Libya can serve as example of cooperation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cover-up: Former Archbishop Lord Hope allowed ...</td>\n",
       "      <td>Very Reverend Robert Waddington sexually abuse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By . Kristie Lau . PUBLISHED: . 10:48 EST, 14 ...</td>\n",
       "      <td>Monday night's episode showed Buddy Valastro t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'The lamps are going out all over Europe. We s...</td>\n",
       "      <td>People asked to turn out lights for hour betwe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 ...   \n",
       "1  UNITED NATIONS (CNN) -- A rare meeting of U.N....   \n",
       "2  Cover-up: Former Archbishop Lord Hope allowed ...   \n",
       "3  By . Kristie Lau . PUBLISHED: . 10:48 EST, 14 ...   \n",
       "4  'The lamps are going out all over Europe. We s...   \n",
       "\n",
       "                                          highlights  \n",
       "0  John and .\\nAudrey Cook were discovered alongs...  \n",
       "1  NEW: Libya can serve as example of cooperation...  \n",
       "2  Very Reverend Robert Waddington sexually abuse...  \n",
       "3  Monday night's episode showed Buddy Valastro t...  \n",
       "4  People asked to turn out lights for hour betwe...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load summarization training data from a CSV file\n",
    "summarization_data = pd.read_csv('Case Dataset/Data files/Summarization_Training.csv', sep=';')\n",
    "\n",
    "summarization_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we define a set of constants that are crucial for performing various NLP tasks such as Named Entity Recognition (NER) and summarization. These constants include pre-trained model identifiers for different tasks and descriptive labels for entities recognized by the NER models. By organizing these constants in a structured manner, we ensure that our code is both readable and easily configurable for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition models\n",
    "NAMED_ENTITY_RECOGNITION_MODELS = {\n",
    "    \"BERT (CoNLL-03 English)\": \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    \"Wikineural Multilingual (Babelscape)\": \"Babelscape/wikineural-multilingual-ner\",\n",
    "    \"BERT (dslim)\": \"dslim/bert-large-NER\",\n",
    "}\n",
    "\n",
    "# Named Entity Recognition labels and descriptions\n",
    "ENTITY_DESCRIPTIONS = {\n",
    "    \"I-PER\": \"Person\",\n",
    "    \"I-ORG\": \"Organization\",\n",
    "    \"I-LOC\": \"Location\",\n",
    "    \"I-MISC\": \"Miscellaneous\",\n",
    "    \"B-PER\": \"Person\",\n",
    "    \"B-ORG\": \"Organization\",\n",
    "    \"B-LOC\": \"Location\",\n",
    "    \"B-MISC\": \"Miscellaneous\"\n",
    "}\n",
    "\n",
    "# Models for summarization tasks\n",
    "SUMMARIZATION_MODELS = {\n",
    "    \"BART (CNN/DailyMail)\": \"facebook/bart-large-cnn\",\n",
    "    \"T5 (CNN/DailyMail)\": \"t5-large\",\n",
    "    \"Pegasus (Newsroom)\": \"google/pegasus-newsroom\",\n",
    "    \"BART (XSum)\": \"facebook/bart-large-xsum\",\n",
    "    \"T5 (XSum)\": \"t5-large\",\n",
    "    \"Pegasus (XSum)\": \"google/pegasus-xsum\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we define a versatile function that handles different types of input sources for our NLP tasks. This function allows users to select input data from direct text input, predefined sample data, or files. By providing a unified approach to input selection, we ensure that our NLP processing pipeline is flexible and user-friendly, accommodating various use cases and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_selection(selected_input: str = None, text: str = None, sample: str = None, file: str = None):\n",
    "    \"\"\"\n",
    "    Selects the input based on the specified input type.\n",
    "\n",
    "    Args:\n",
    "        selected_input (str): The type of input, either \"text\", \"sample\", or \"file\".\n",
    "        text (str): The text input for processing.\n",
    "        sample (str): The sample input key for retrieving sample data.\n",
    "        file (File): The file object containing the input data.\n",
    "\n",
    "    Returns:\n",
    "        str: The selected input data as a string.\n",
    "    \"\"\"\n",
    "    if selected_input == \"text\":\n",
    "        retval = text\n",
    "    elif selected_input == \"sample\":\n",
    "        retval = named_entity_recognition_data[sample] if sample in named_entity_recognition_data else sample\n",
    "    elif selected_input == \"file\":\n",
    "        with open(file.name, 'r') as f:\n",
    "            retval = f.read()\n",
    "    else:\n",
    "        retval = None\n",
    "    \n",
    "    return retval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a crucial task in Natural Language Processing (NLP) that involves identifying and classifying entities in text into predefined categories such as persons, organizations, locations, and more. In this section, we implement NER using pre-trained models and provide functionality for comparing the outputs of two different models. This implementation includes generating a legend of identified entities, which enhances the interpretability of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_legend(entities):\n",
    "    \"\"\"\n",
    "    Generates a legend string for the provided entities.\n",
    "\n",
    "    Args:\n",
    "        entities (list): A list of dictionaries containing entity information.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string that represents the legend of entities.\n",
    "    \"\"\"\n",
    "    legend = {}\n",
    "    for ent in entities:\n",
    "        if ent['entity'] not in legend:\n",
    "            legend[ent['entity']] = 1\n",
    "        else:\n",
    "            legend[ent['entity']] += 1\n",
    "\n",
    "    legend_str = \"Entities Legend:\\n\" + \"\\n\".join(\n",
    "        [f\"{ENTITY_DESCRIPTIONS.get(entity, entity)} ({entity}): {count}\" for entity, count in legend.items()]\n",
    "    )\n",
    "    return legend_str\n",
    "\n",
    "\n",
    "def named_entity_recognition(model: str = None, model_to_compare: str = None,\n",
    "                             selected_input: str = \"text\", text: str = None,\n",
    "                             sample: str = None, file: str = None):\n",
    "    \"\"\"\n",
    "    Performs named entity recognition (NER) using the specified models and inputs.\n",
    "\n",
    "    Args:\n",
    "        model (str): The primary model to use for NER.\n",
    "        model_to_compare (str): The secondary model to compare results with.\n",
    "        selected_input (str): The type of input, e.g., \"text\", \"sample\", or \"file\".\n",
    "        text (str): The text input for NER analysis.\n",
    "        sample (str): The sample input for NER analysis.\n",
    "        file (str): The file input for NER analysis.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing NER results and their legends for both models.\n",
    "    \"\"\"\n",
    "    entities = {}\n",
    "    models = [model, model_to_compare]\n",
    "    text = input_selection(selected_input, text, sample, file)\n",
    "    \n",
    "    for model_name in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NAMED_ENTITY_RECOGNITION_MODELS[model_name], model_max_length=512)\n",
    "        model = NAMED_ENTITY_RECOGNITION_MODELS[model_name]\n",
    "        nlp = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "        model_entities = nlp(text)\n",
    "        entities[model_name] = [{\"entity\": ent[\"entity\"], \"score\": ent[\"score\"], \"index\": ent[\"index\"],\n",
    "                                 \"start\": ent[\"start\"], \"end\": ent[\"end\"]} for ent in model_entities]\n",
    "\n",
    "    legend_text_1 = generate_legend(entities[models[0]])\n",
    "    legend_text_2 = generate_legend(entities[models[1]])\n",
    "    \n",
    "    return ({\"text\": text, \"entities\": entities[models[0]]}, legend_text_1,\n",
    "            {\"text\": text, \"entities\": entities[models[1]]}, legend_text_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement translation functionalities using state-of-the-art NLP models from Helsinki-NLP and Facebook. These models facilitate the translation of text from one language to another. The implementation includes functions for translating text using each of these models and a function to compare translations from both models along with a provided reference translation. This setup ensures a robust and comprehensive translation pipeline that leverages the strengths of different translation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helsinki_translation(source_language: str = None, target_language: str = None, text: str = None):\n",
    "    \"\"\"\n",
    "    Translates text using the Helsinki-NLP translation model.\n",
    "\n",
    "    Args:\n",
    "        source_language (str): The source language code.\n",
    "        target_language (str): The target language code.\n",
    "        text (str): The text to be translated.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    helsinki_translation_pipeline = pipeline(\n",
    "        'translation', model=f\"Helsinki-NLP/opus-mt-{source_language}-{target_language}\"\n",
    "    )\n",
    "    helsinki_translation = helsinki_translation_pipeline(text)\n",
    "    return helsinki_translation[0]['translation_text']\n",
    "\n",
    "\n",
    "def facebook_translation(source_language: str = None, target_language: str = None, text: str = None):\n",
    "    \"\"\"\n",
    "    Translates text using the Facebook M2M100 translation model.\n",
    "\n",
    "    Args:\n",
    "        source_language (str): The source language code.\n",
    "        target_language (str): The target language code.\n",
    "        text (str): The text to be translated.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    facebook_translation_pipeline = pipeline(\n",
    "        'translation', model=\"facebook/m2m100_418M\", src_lang=source_language, tgt_lang=target_language\n",
    "    )\n",
    "    facebook_translation = facebook_translation_pipeline(text)\n",
    "    return facebook_translation[0]['translation_text']\n",
    "\n",
    "\n",
    "def translation(source_language: str = None, target_language: str = None, selected_input: str = None,\n",
    "                text: str = None, sample: str = None, file: str = None):\n",
    "    \"\"\"\n",
    "    Translates text using both Helsinki-NLP and Facebook translation models and compares with provided translations.\n",
    "\n",
    "    Args:\n",
    "        source_language (str): The source language code.\n",
    "        target_language (str): The target language code.\n",
    "        selected_input (str): The type of input, e.g., \"text\", \"sample\", or \"file\".\n",
    "        text (str): The text input for translation.\n",
    "        sample (str): The sample input for translation.\n",
    "        file (str): The file input for translation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The translations from Helsinki-NLP, Facebook, and the provided translation data.\n",
    "    \"\"\"\n",
    "    input_text = input_selection(selected_input, text, sample, file)\n",
    "    first_translation = helsinki_translation(source_language=source_language, target_language=target_language, text=input_text)\n",
    "    second_translation = facebook_translation(source_language=source_language, target_language=target_language, text=input_text)\n",
    "    provided_translation = translation_data.loc[translation_data[source_language] == input_text, target_language].values[0]\n",
    "    return first_translation, second_translation, provided_translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization is a vital task in Natural Language Processing (NLP) that aims to condense large bodies of text into shorter, coherent summaries, capturing the main points and essential information. In this section, we implement a summarization functionality using pre-trained models, enabling the generation of concise summaries from various input sources. This implementation includes handling long texts by chunking them and comparing generated summaries with provided reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(tokenizer, text, max_length):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of a specified maximum length using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use for splitting text.\n",
    "        text (str): The text to be split into chunks.\n",
    "        max_length (int): The maximum length of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of token chunks, each of max_length or smaller.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "\n",
    "def summarization(model: str = None, selected_input: str = None, text: str = None, sample: str = None, file: str = None):\n",
    "    \"\"\"\n",
    "    Summarizes the given text using the specified summarization model.\n",
    "\n",
    "    Args:\n",
    "        model (str): The model to use for summarization.\n",
    "        selected_input (str): The type of input, e.g., \"text\", \"sample\", or \"file\".\n",
    "        text (str): The text input for summarization.\n",
    "        sample (str): The sample input for summarization.\n",
    "        file (str): The file input for summarization.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The generated summary and the provided summary from the dataset.\n",
    "    \"\"\"\n",
    "    text = input_selection(selected_input, text, sample, file)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(SUMMARIZATION_MODELS[model])\n",
    "    chunks = chunk_text(tokenizer, text, 512)\n",
    "    summarization_pipeline = pipeline('summarization', model=SUMMARIZATION_MODELS[model])\n",
    "    \n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        chunked_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        summary = summarization_pipeline(chunked_text)\n",
    "        summaries.append(summary[0]['summary_text'])\n",
    "    \n",
    "    summary_text = ' '.join(summaries)\n",
    "    if selected_input == \"sample\":\n",
    "        provided_summary = summarization_data.loc[summarization_data['article'] == sample, 'highlights'].values[0]\n",
    "    else:\n",
    "        provided_summary = \"\"\n",
    "    return summary_text, provided_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio is a powerful library that allows developers to quickly create interactive web interfaces for machine learning models. In this section, we implement Gradio interfaces for three core NLP tasks: Named Entity Recognition (NER), Translation, and Summarization. Each interface allows users to interact with pre-trained models by providing inputs directly, choosing from predefined samples, or uploading files. The implementation ensures that the user experience is seamless and intuitive, making it easy to explore and compare the outputs of different NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_legend_and_output(model, model_to_compare, selected_input, text, samples, file):\n",
    "    \"\"\"\n",
    "    Updates the legend and output for named entity recognition.\n",
    "\n",
    "    Args:\n",
    "        model (str): The model to use for NER.\n",
    "        model_to_compare (str): The model to compare against.\n",
    "        selected_input (str): The type of input, e.g., \"text\", \"sample\", or \"file\".\n",
    "        text (str): The text input for NER.\n",
    "        samples (str): The sample input for NER.\n",
    "        file (str): The file input for NER.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The highlighted text and legend for both models.\n",
    "    \"\"\"\n",
    "    highlighted_text_output_1, legend_text_1, highlighted_text_output_2, legend_text_2 = named_entity_recognition(\n",
    "        model, model_to_compare, selected_input, text, samples, file\n",
    "    )\n",
    "    return highlighted_text_output_1, legend_text_1, highlighted_text_output_2, legend_text_2\n",
    "\n",
    "def named_entity_recognition_interface():\n",
    "    \"\"\"\n",
    "    Creates the named entity recognition interface using Gradio.\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: The Gradio interface for named entity recognition.\n",
    "    \"\"\"\n",
    "    with gr.Blocks() as blocks:\n",
    "        model = gr.Dropdown(list(NAMED_ENTITY_RECOGNITION_MODELS.keys()), label=\"Model\", value=\"BERT (CoNLL-03 English)\")\n",
    "        model_to_compare = gr.Dropdown(list(NAMED_ENTITY_RECOGNITION_MODELS.keys()), label=\"Model to Compare\", value=\"Wikineural Multilingual (Babelscape)\")\n",
    "        text = gr.Textbox(lines=5, label=\"Input\", value=\"Please enter the text to analyze.\")\n",
    "        samples = gr.Dropdown(list(named_entity_recognition_data.keys()), label=\"Samples\", value=\"Introduction\")\n",
    "        file = gr.File(label=\"Upload a file\")\n",
    "        selected_input = gr.Radio(\n",
    "            [(\"Text\", \"text\"), (\"Sample\", \"sample\"), (\"File\", \"file\")], \n",
    "            label=\"Select Input Type\", \n",
    "            value=\"sample\"\n",
    "        )\n",
    "\n",
    "        highlighted_text_1 = gr.HighlightedText(label=\"Model 1 Result\")\n",
    "        legend_1 = gr.Markdown(\"Entities Legend for Model 1 will be shown here\")\n",
    "        \n",
    "        highlighted_text_2 = gr.HighlightedText(label=\"Model 2 Result\")\n",
    "        legend_2 = gr.Markdown(\"Entities Legend for Model 2 will be shown here\")\n",
    "\n",
    "        inputs = [model, model_to_compare, selected_input, text, samples, file]\n",
    "        for input_component in inputs:\n",
    "            input_component.change(\n",
    "                update_legend_and_output, \n",
    "                inputs=inputs, \n",
    "                outputs=[highlighted_text_1, legend_1, highlighted_text_2, legend_2]\n",
    "            )\n",
    "\n",
    "        gr.Interface(\n",
    "            fn=update_legend_and_output, \n",
    "            inputs=inputs, \n",
    "            outputs=[highlighted_text_1, legend_1, highlighted_text_2, legend_2], \n",
    "            title=\"Named Entity Recognition\"\n",
    "        )\n",
    "        \n",
    "    return blocks\n",
    "\n",
    "def update_samples(source_language):\n",
    "    \"\"\"\n",
    "    Updates the sample choices based on the selected source language.\n",
    "\n",
    "    Args:\n",
    "        source_language (str): The selected source language.\n",
    "\n",
    "    Returns:\n",
    "        gr.update: Updated dropdown choices for samples.\n",
    "    \"\"\"\n",
    "    samples = list(translation_data[source_language])\n",
    "    return gr.update(choices=samples, value=samples[0])\n",
    "\n",
    "def translation_interface():\n",
    "    \"\"\"\n",
    "    Creates the translation interface using Gradio.\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: The Gradio interface for translation.\n",
    "    \"\"\"\n",
    "    with gr.Blocks() as blocks:\n",
    "        source_language = gr.Dropdown(\n",
    "            choices=[(\"English\", \"en\"), (\"Spanish\", \"es\"), (\"French\", \"fr\"), (\"German\", \"de\"), (\"Italian\", \"it\")], \n",
    "            label=\"Source Language\", \n",
    "            multiselect=False, \n",
    "            value=\"en\"\n",
    "        )\n",
    "\n",
    "        target_language = gr.Dropdown(\n",
    "            choices=[(\"English\", \"en\"), (\"Spanish\", \"es\"), (\"French\", \"fr\"), (\"German\", \"de\"), (\"Italian\", \"it\")], \n",
    "            label=\"Target Language\", \n",
    "            multiselect=False, \n",
    "            value=\"de\"\n",
    "        )\n",
    "\n",
    "        text = gr.Textbox(lines=5, label=\"Input\", value=\"Please enter the text to translate.\")\n",
    "\n",
    "        sample = gr.Dropdown(\n",
    "            choices=list(translation_data[\"en\"]), \n",
    "            label=\"Samples\", \n",
    "            value=translation_data[\"en\"][0]\n",
    "        )\n",
    "\n",
    "        file = gr.File(label=\"Upload a file\")\n",
    "\n",
    "        selected_input = gr.Radio(\n",
    "            choices=[(\"Text\", \"text\"), (\"Sample\", \"sample\"), (\"File\", \"file\")], \n",
    "            label=\"Select Input Type\", \n",
    "            value=\"sample\"\n",
    "        )\n",
    "\n",
    "        source_language.change(update_samples, inputs=source_language, outputs=sample)\n",
    "\n",
    "        gr.Interface(\n",
    "            fn=translation, \n",
    "            inputs=[source_language, target_language, selected_input, text, sample, file], \n",
    "            outputs=[gr.Textbox(label=\"Translation\"), gr.Textbox(label=\"Alternative\"), gr.Textbox(label=\"Ground Truth\")], \n",
    "            title=\"Translation\"\n",
    "        )\n",
    "        \n",
    "    return blocks\n",
    "\n",
    "def summarization_interface():\n",
    "    \"\"\"\n",
    "    Creates the summarization interface using Gradio.\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: The Gradio interface for summarization.\n",
    "    \"\"\"\n",
    "    with gr.Blocks() as blocks:\n",
    "        model = gr.Dropdown(list(SUMMARIZATION_MODELS.keys()), label=\"Model\", value=\"BART (CNN/DailyMail)\")\n",
    "        \n",
    "        text = gr.Textbox(lines=5, label=\"Input\", value=\"Please enter the text to summarize.\")\n",
    "        \n",
    "        sample = gr.Dropdown(list(summarization_data[\"article\"]), label=\"Samples\", value=summarization_data[\"article\"][0])\n",
    "        \n",
    "        file = gr.File(label=\"Upload a file\")\n",
    "        \n",
    "        selected_input = gr.Radio(\n",
    "            choices=[(\"Text\", \"text\"), (\"Sample\", \"sample\"), (\"File\", \"file\")], \n",
    "            label=\"Select Input Type\", \n",
    "            value=\"sample\"\n",
    "        )\n",
    "\n",
    "        gr.Interface(\n",
    "            fn=summarization, \n",
    "            inputs=[model, selected_input, text, sample, file], \n",
    "            outputs=[gr.Textbox(label=\"Summary\"), gr.Textbox(label=\"Highlights (Ground Truth)\")], \n",
    "            title=\"Summarization\"\n",
    "        )\n",
    "        \n",
    "    return blocks\n",
    "\n",
    "def build_interface():\n",
    "    \"\"\"\n",
    "    Builds the entire Gradio interface with tabs for named entity recognition, translation, and summarization.\n",
    "\n",
    "    Returns:\n",
    "        gr.Interface: The complete Gradio interface.\n",
    "    \"\"\"\n",
    "    interface = gr.TabbedInterface([\n",
    "        named_entity_recognition_interface(),\n",
    "        translation_interface(),\n",
    "        summarization_interface()\n",
    "    ], [\"Named Entity Recognition\", \"Translation\", \"Summarization\"], title=\"NLP Toolkit\")\n",
    "    return interface\n",
    "\n",
    "interface = build_interface()\n",
    "interface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural-language-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
